{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2AScit-Dr3as"
   },
   "source": [
    "# **Modelo MLP con `PySpark`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "N8RVc520agvw",
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in c:\\users\\hp\\anaconda3\\envs\\machine\\lib\\site-packages (4.0.0)\n",
      "Requirement already satisfied: py4j==0.10.9.9 in c:\\users\\hp\\anaconda3\\envs\\machine\\lib\\site-packages (from pyspark) (0.10.9.9)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "4smZDrXp-sNM",
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "El sistema no puede encontrar la ruta especificada.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\"wget\" no se reconoce como un comando interno o externo,\n",
      "programa o archivo por lotes ejecutable.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tar: Error opening archive: Failed to open 'spark-3.4.0-bin-hadoop3.tgz'\n"
     ]
    }
   ],
   "source": [
    "# INSTALACIÃ“N Y CONFIGURACIÃ“N\n",
    "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
    "!wget -q https://archive.apache.org/dist/spark/spark-3.4.0/spark-3.4.0-bin-hadoop3.tgz\n",
    "!tar xf spark-3.4.0-bin-hadoop3.tgz\n",
    "!pip install -q findspark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "x7DDo8VLp8AN",
    "notebookRunGroups": {
     "groupValue": "2"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Librerias\n",
    "import time\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler, Imputer\n",
    "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.sql.functions import col, when\n",
    "import numpy as np\n",
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.4.0-bin-hadoop3\"\n",
    "\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "JMrXv24zuhFh",
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    from pyspark.ml.functions import vector_to_array\n",
    "except Exception:\n",
    "    from pyspark.sql.functions import vector_to_array  # fallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "2KEi3_PKar2Y",
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 2] El sistema no puede encontrar el archivo especificado",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Iniciar sesiÃ³n Spark\u001b[39;00m\n\u001b[0;32m      2\u001b[0m spark \u001b[38;5;241m=\u001b[39m \u001b[43mSparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuilder\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappName\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMyMini\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m----> 4\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\MACHINE\\lib\\site-packages\\pyspark\\sql\\session.py:556\u001b[0m, in \u001b[0;36mSparkSession.Builder.getOrCreate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    554\u001b[0m     sparkConf\u001b[38;5;241m.\u001b[39mset(key, value)\n\u001b[0;32m    555\u001b[0m \u001b[38;5;66;03m# This SparkContext may be an existing one.\u001b[39;00m\n\u001b[1;32m--> 556\u001b[0m sc \u001b[38;5;241m=\u001b[39m \u001b[43mSparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msparkConf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    557\u001b[0m \u001b[38;5;66;03m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[0;32m    558\u001b[0m \u001b[38;5;66;03m# by all sessions.\u001b[39;00m\n\u001b[0;32m    559\u001b[0m session \u001b[38;5;241m=\u001b[39m SparkSession(sc, options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\MACHINE\\lib\\site-packages\\pyspark\\core\\context.py:523\u001b[0m, in \u001b[0;36mSparkContext.getOrCreate\u001b[1;34m(cls, conf)\u001b[0m\n\u001b[0;32m    521\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    522\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 523\u001b[0m         \u001b[43mSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mSparkConf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    524\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    525\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\MACHINE\\lib\\site-packages\\pyspark\\core\\context.py:205\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gateway \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m gateway\u001b[38;5;241m.\u001b[39mgateway_parameters\u001b[38;5;241m.\u001b[39mauth_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    200\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    201\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    202\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is not allowed as it is a security risk.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    203\u001b[0m     )\n\u001b[1;32m--> 205\u001b[0m \u001b[43mSparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ensure_initialized\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgateway\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgateway\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    207\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_init(\n\u001b[0;32m    208\u001b[0m         master,\n\u001b[0;32m    209\u001b[0m         appName,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    219\u001b[0m         memory_profiler_cls,\n\u001b[0;32m    220\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\MACHINE\\lib\\site-packages\\pyspark\\core\\context.py:444\u001b[0m, in \u001b[0;36mSparkContext._ensure_initialized\u001b[1;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[0;32m    442\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    443\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_gateway:\n\u001b[1;32m--> 444\u001b[0m         SparkContext\u001b[38;5;241m.\u001b[39m_gateway \u001b[38;5;241m=\u001b[39m gateway \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mlaunch_gateway\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    445\u001b[0m         SparkContext\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39m_gateway\u001b[38;5;241m.\u001b[39mjvm\n\u001b[0;32m    447\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m instance:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\MACHINE\\lib\\site-packages\\pyspark\\java_gateway.py:104\u001b[0m, in \u001b[0;36mlaunch_gateway\u001b[1;34m(conf, popen_kwargs)\u001b[0m\n\u001b[0;32m    101\u001b[0m     proc \u001b[38;5;241m=\u001b[39m Popen(command, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpopen_kwargs)\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    103\u001b[0m     \u001b[38;5;66;03m# preexec_fn not supported on Windows\u001b[39;00m\n\u001b[1;32m--> 104\u001b[0m     proc \u001b[38;5;241m=\u001b[39m Popen(command, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpopen_kwargs)\n\u001b[0;32m    106\u001b[0m \u001b[38;5;66;03m# Wait for the file to appear, or for the process to exit, whichever happens first.\u001b[39;00m\n\u001b[0;32m    107\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m proc\u001b[38;5;241m.\u001b[39mpoll() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(conn_info_file):\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\MACHINE\\lib\\subprocess.py:971\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[1;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask, pipesize)\u001b[0m\n\u001b[0;32m    967\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_mode:\n\u001b[0;32m    968\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mTextIOWrapper(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr,\n\u001b[0;32m    969\u001b[0m                     encoding\u001b[38;5;241m=\u001b[39mencoding, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[1;32m--> 971\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecutable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreexec_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclose_fds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    972\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mpass_fds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcwd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    973\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mstartupinfo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreationflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshell\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    974\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mp2cread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp2cwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    975\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mc2pread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc2pwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    976\u001b[0m \u001b[43m                        \u001b[49m\u001b[43merrread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    977\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mrestore_signals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    978\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mgid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mumask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    979\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mstart_new_session\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    980\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m    981\u001b[0m     \u001b[38;5;66;03m# Cleanup if the child failed starting.\u001b[39;00m\n\u001b[0;32m    982\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mfilter\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m, (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstdin, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstdout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr)):\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\MACHINE\\lib\\subprocess.py:1456\u001b[0m, in \u001b[0;36mPopen._execute_child\u001b[1;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, unused_restore_signals, unused_gid, unused_gids, unused_uid, unused_umask, unused_start_new_session)\u001b[0m\n\u001b[0;32m   1454\u001b[0m \u001b[38;5;66;03m# Start the process\u001b[39;00m\n\u001b[0;32m   1455\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1456\u001b[0m     hp, ht, pid, tid \u001b[38;5;241m=\u001b[39m \u001b[43m_winapi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCreateProcess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexecutable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1457\u001b[0m \u001b[43m                             \u001b[49m\u001b[38;5;66;43;03m# no special security\u001b[39;49;00m\n\u001b[0;32m   1458\u001b[0m \u001b[43m                             \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1459\u001b[0m \u001b[43m                             \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mclose_fds\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1460\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mcreationflags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1461\u001b[0m \u001b[43m                             \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1462\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mcwd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1463\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mstartupinfo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1464\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m   1465\u001b[0m     \u001b[38;5;66;03m# Child is launched. Close the parent's copy of those pipe\u001b[39;00m\n\u001b[0;32m   1466\u001b[0m     \u001b[38;5;66;03m# handles that only the child should have open.  You need\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1469\u001b[0m     \u001b[38;5;66;03m# pipe will not close when the child process exits and the\u001b[39;00m\n\u001b[0;32m   1470\u001b[0m     \u001b[38;5;66;03m# ReadFile will hang.\u001b[39;00m\n\u001b[0;32m   1471\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_pipe_fds(p2cread, p2cwrite,\n\u001b[0;32m   1472\u001b[0m                          c2pread, c2pwrite,\n\u001b[0;32m   1473\u001b[0m                          errread, errwrite)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 2] El sistema no puede encontrar el archivo especificado"
     ]
    }
   ],
   "source": [
    "# Iniciar sesiÃ³n Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MyMini\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HGl9WRdcAVPA",
    "outputId": "324a7718-79d1-4a65-dacc-69f6eabd3b8f",
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recursos Colab:\n",
      "   â€¢ RAM total: 12.7 GB\n",
      "   â€¢ RAM disponible: 11.0 GB\n",
      "   â€¢ Cores CPU: 2\n",
      "   â€¢ Spark cores: 2\n"
     ]
    }
   ],
   "source": [
    "# MONITOREO DE RECURSOS\n",
    "def check_colab_resources():\n",
    "    \"\"\"Verificar recursos disponibles en Colab\"\"\"\n",
    "    import psutil\n",
    "    memory = psutil.virtual_memory()\n",
    "    print(f\"Recursos Colab:\")\n",
    "    print(f\"   â€¢ RAM total: {memory.total / (1024**3):.1f} GB\")\n",
    "    print(f\"   â€¢ RAM disponible: {memory.available / (1024**3):.1f} GB\")\n",
    "    print(f\"   â€¢ Cores CPU: {os.cpu_count()}\")\n",
    "    print(f\"   â€¢ Spark cores: {spark.sparkContext.defaultParallelism}\")\n",
    "\n",
    "check_colab_resources()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l684sCLzEFoW"
   },
   "source": [
    "### Funciones de evaluaciÃ³n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "llxrOWGmDCC4",
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "def evaluate_model_comprehensive(model, train_df, test_df, labelCol=\"label\"):\n",
    "    \"\"\"\n",
    "    EvaluaciÃ³n completa del modelo con mÃºltiples mÃ©tricas\n",
    "    en ambos conjuntos de entrenamiento y prueba\n",
    "    \"\"\"\n",
    "\n",
    "    # Hacer predicciones\n",
    "    train_predictions = model.transform(train_df)\n",
    "    test_predictions = model.transform(test_df)\n",
    "\n",
    "    # Evaluadores para mÃ©tricas binarias\n",
    "    evaluator_auc_roc = BinaryClassificationEvaluator(\n",
    "        labelCol=labelCol,\n",
    "        rawPredictionCol=\"rawPrediction\",\n",
    "        metricName=\"areaUnderROC\"\n",
    "    )\n",
    "    evaluator_auc_pr = BinaryClassificationEvaluator(\n",
    "        labelCol=labelCol,\n",
    "        rawPredictionCol=\"rawPrediction\",\n",
    "        metricName=\"areaUnderPR\"\n",
    "    )\n",
    "    # Evaluadores para mÃ©tricas multiclase\n",
    "    evaluator_f1 = MulticlassClassificationEvaluator(\n",
    "        labelCol=labelCol,\n",
    "        predictionCol=\"prediction\",\n",
    "        metricName=\"f1\"\n",
    "    )\n",
    "    evaluator_precision = MulticlassClassificationEvaluator(\n",
    "        labelCol=labelCol,\n",
    "        predictionCol=\"prediction\",\n",
    "        metricName=\"weightedPrecision\"\n",
    "    )\n",
    "    evaluator_recall = MulticlassClassificationEvaluator(\n",
    "        labelCol=labelCol,\n",
    "        predictionCol=\"prediction\",\n",
    "        metricName=\"weightedRecall\"\n",
    "    )\n",
    "    evaluator_accuracy = MulticlassClassificationEvaluator(\n",
    "        labelCol=labelCol,\n",
    "        predictionCol=\"prediction\",\n",
    "        metricName=\"accuracy\"\n",
    "    )\n",
    "\n",
    "    # Calcular mÃ©tricas para TRAIN\n",
    "    train_auc_roc = evaluator_auc_roc.evaluate(train_predictions)\n",
    "    train_auc_pr = evaluator_auc_pr.evaluate(train_predictions)\n",
    "    train_f1 = evaluator_f1.evaluate(train_predictions)\n",
    "    train_precision = evaluator_precision.evaluate(train_predictions)\n",
    "    train_recall = evaluator_recall.evaluate(train_predictions)\n",
    "    train_accuracy = evaluator_accuracy.evaluate(train_predictions)\n",
    "\n",
    "    # Calcular mÃ©tricas para TEST\n",
    "    test_auc_roc = evaluator_auc_roc.evaluate(test_predictions)\n",
    "    test_auc_pr = evaluator_auc_pr.evaluate(test_predictions)\n",
    "    test_f1 = evaluator_f1.evaluate(test_predictions)\n",
    "    test_precision = evaluator_precision.evaluate(test_predictions)\n",
    "    test_recall = evaluator_recall.evaluate(test_predictions)\n",
    "    test_accuracy = evaluator_accuracy.evaluate(test_predictions)\n",
    "\n",
    "    # Calcular matriz de confusiÃ³n para test\n",
    "    confusion_matrix = test_predictions.groupBy(\n",
    "        F.col(labelCol).alias(\"Actual\"),\n",
    "        F.col(\"prediction\").alias(\"Predicted\")\n",
    "    ).count().orderBy(\"Actual\", \"Predicted\")\n",
    "\n",
    "    return {\n",
    "        'train_metrics': {\n",
    "            'AUC_ROC': train_auc_roc,\n",
    "            'AUC_PR': train_auc_pr,\n",
    "            'F1': train_f1,\n",
    "            'Precision': train_precision,\n",
    "            'Recall': train_recall,\n",
    "            'Accuracy': train_accuracy\n",
    "        },\n",
    "        'test_metrics': {\n",
    "            'AUC_ROC': test_auc_roc,\n",
    "            'AUC_PR': test_auc_pr,\n",
    "            'F1': test_f1,\n",
    "            'Precision': test_precision,\n",
    "            'Recall': test_recall,\n",
    "            'Accuracy': test_accuracy\n",
    "        },\n",
    "        'confusion_matrix': confusion_matrix\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0-D7R4PnDfb5",
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Calcular mÃ©tricas especÃ­ficas por clase desde matriz de confusiÃ³n\n",
    "def calculate_detailed_metrics(confusion_df):\n",
    "    \"\"\"Calcular mÃ©tricas detalladas por clase desde matriz de confusiÃ³n\"\"\"\n",
    "    # Asumiendo que las clases son 0.0 y 1.0\n",
    "    try:\n",
    "        tn = confusion_df[(confusion_df['Actual'] == 0.0) & (confusion_df['Predicted'] == 0.0)]['count'].values[0]\n",
    "        fp = confusion_df[(confusion_df['Actual'] == 0.0) & (confusion_df['Predicted'] == 1.0)]['count'].values[0]\n",
    "        fn = confusion_df[(confusion_df['Actual'] == 1.0) & (confusion_df['Predicted'] == 0.0)]['count'].values[0]\n",
    "        tp = confusion_df[(confusion_df['Actual'] == 1.0) & (confusion_df['Predicted'] == 1.0)]['count'].values[0]\n",
    "\n",
    "        precision_1 = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall_1 = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1_1 = 2 * (precision_1 * recall_1) / (precision_1 + recall_1) if (precision_1 + recall_1) > 0 else 0\n",
    "\n",
    "        precision_0 = tn / (tn + fn) if (tn + fn) > 0 else 0\n",
    "        recall_0 = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "        f1_0 = 2 * (precision_0 * recall_0) / (precision_0 + recall_0) if (precision_0 + recall_0) > 0 else 0\n",
    "\n",
    "        return {\n",
    "            'class_0': {'precision': precision_0, 'recall': recall_0, 'f1': f1_0},\n",
    "            'class_1': {'precision': precision_1, 'recall': recall_1, 'f1': f1_1}\n",
    "        }\n",
    "    except:\n",
    "        return \"No se pudo calcular mÃ©tricas detalladas\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yxyExIzv6R_r"
   },
   "source": [
    "## Encontrar los mejores hiperparÃ¡metros\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4IXgZYXK6XKu"
   },
   "source": [
    "Uso de CrossValidation de 3 folds.\n",
    "Se utilizo el 10% de la data dispobible.\n",
    "\n",
    "La metrica de evaluaciÃ³n fue `areaUnderPR`: 'area bajo la curva presiciÃ³n recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-H55y_jxxUTa"
   },
   "outputs": [],
   "source": [
    "# ğŸ”¹ Lectura de datos [10% de los datos]\n",
    "df = spark.read.csv(\"data_to_model.csv\", header=True, inferSchema=True)\n",
    "df = df.sample(fraction=0.1, seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ConfiguraciÃ³n inicial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3AR4HKqFz_cd",
    "outputId": "61536b8d-6888-479f-ea7e-beda450e235a",
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variables categÃ³ricas: ['motivo_prestamo', 'tipo_vivienda']\n",
      "Variables numÃ©ricas: ['monto_aprobado', 'tasa_interes', 'plazo_meses', 'ingreso_anual', 'antiguedad_laboral', 'estado_verif_ingreso', 'promedio_fico', 'anio_apertura_credito', 'cuentas_hipotecarias', 'total_cuentas_credito', 'cuentas_tarjeta_credito', 'saldo_revolvente', 'uso_credito_revolvente', 'limite_credito_total', 'dti', 'meses_ultima_consulta', 'meses_tarjeta_nueva', 'lineas_credito_12m', 'porcentaje_sin_moras', 'moras_2_year', 'monto_total_cobranzas', 'tuvo_acuerdo_pago', 'subcategoria_credito', 'anio_emision_prestamo', 'mes_emision_prestamo']\n"
     ]
    }
   ],
   "source": [
    "# ğŸ”¹ ConfiguraciÃ³n inicial\n",
    "label_col = \"default\"\n",
    "\n",
    "# DetecciÃ³n automÃ¡tica de columnas\n",
    "categorical_cols = [\n",
    "    col for col in df.columns\n",
    "    if df.schema[col].dataType.typeName() == 'string' and col != label_col]\n",
    "numeric_cols = [\n",
    "    col for col in df.columns\n",
    "    if df.schema[col].dataType.typeName() in ['integer', 'double', 'float'] and col != label_col]\n",
    "\n",
    "print(f\"Variables categÃ³ricas: {categorical_cols}\")\n",
    "print(f\"Variables numÃ©ricas: {numeric_cols}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Preprocesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tZuNBNWr0FZb",
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# ğŸ”¹ Preprocesamiento\n",
    "\n",
    "# Convertir label a DoubleType y filtrar nulos\n",
    "df = df.withColumn(\"label\", F.col(label_col).cast(DoubleType()))\\\n",
    "       .filter(F.col(\"label\").isNotNull())\n",
    "\n",
    "# ImputaciÃ³n de valores nulos en variables numÃ©ricas\n",
    "imputer = Imputer(\n",
    "    inputCols=numeric_cols,\n",
    "    outputCols=[f\"{c}_imp\" for c in numeric_cols],\n",
    "    strategy=\"median\"\n",
    ")\n",
    "num_imp_cols = [f\"{c}_imp\" for c in numeric_cols]\n",
    "\n",
    "# Indexado y encoding de variables categÃ³ricas\n",
    "indexers = [StringIndexer(inputCol=c, outputCol=f\"{c}_idx\", handleInvalid=\"keep\")\n",
    "            for c in categorical_cols]\n",
    "\n",
    "encoder = OneHotEncoder(\n",
    "    inputCols=[f\"{c}_idx\" for c in categorical_cols],\n",
    "    outputCols=[f\"{c}_oh\" for c in categorical_cols]\n",
    ")\n",
    "ohe_cols = [f\"{c}_oh\" for c in categorical_cols]\n",
    "\n",
    "# Assembler y scaler\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=num_imp_cols + ohe_cols,\n",
    "    outputCol=\"features_raw\",\n",
    "    handleInvalid=\"keep\"\n",
    ")\n",
    "\n",
    "scaler = StandardScaler(\n",
    "    inputCol=\"features_raw\",\n",
    "    outputCol=\"features\",\n",
    "    withStd=True,\n",
    "    withMean=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "bJo17sbE0UcH",
    "outputId": "467b4839-9cad-4260-cf09-72ecdadd46f9",
    "tags": [
     "remove-input",
     "hide-output"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paso 1: Imputer\n",
      "inputCol: input column name. (undefined)\n",
      "inputCols: input column names. (current: ['monto_aprobado', 'tasa_interes', 'plazo_meses', 'ingreso_anual', 'antiguedad_laboral', 'estado_verif_ingreso', 'promedio_fico', 'anio_apertura_credito', 'cuentas_hipotecarias', 'total_cuentas_credito', 'cuentas_tarjeta_credito', 'saldo_revolvente', 'uso_credito_revolvente', 'limite_credito_total', 'dti', 'meses_ultima_consulta', 'meses_tarjeta_nueva', 'lineas_credito_12m', 'porcentaje_sin_moras', 'moras_2_year', 'monto_total_cobranzas', 'tuvo_acuerdo_pago', 'subcategoria_credito', 'anio_emision_prestamo', 'mes_emision_prestamo'])\n",
      "missingValue: The placeholder for the missing values. All occurrences of missingValue will be imputed. (default: nan)\n",
      "outputCol: output column name. (default: Imputer_78adc9146dfc__output)\n",
      "outputCols: output column names. (current: ['monto_aprobado_imp', 'tasa_interes_imp', 'plazo_meses_imp', 'ingreso_anual_imp', 'antiguedad_laboral_imp', 'estado_verif_ingreso_imp', 'promedio_fico_imp', 'anio_apertura_credito_imp', 'cuentas_hipotecarias_imp', 'total_cuentas_credito_imp', 'cuentas_tarjeta_credito_imp', 'saldo_revolvente_imp', 'uso_credito_revolvente_imp', 'limite_credito_total_imp', 'dti_imp', 'meses_ultima_consulta_imp', 'meses_tarjeta_nueva_imp', 'lineas_credito_12m_imp', 'porcentaje_sin_moras_imp', 'moras_2_year_imp', 'monto_total_cobranzas_imp', 'tuvo_acuerdo_pago_imp', 'subcategoria_credito_imp', 'anio_emision_prestamo_imp', 'mes_emision_prestamo_imp'])\n",
      "relativeError: the relative target precision for the approximate quantile algorithm. Must be in the range [0, 1] (default: 0.001)\n",
      "strategy: strategy for imputation. If mean, then replace missing values using the mean value of the feature. If median, then replace missing values using the median value of the feature. If mode, then replace missing using the most frequent value of the feature. (default: mean, current: median)\n",
      "--------------------------------------------------\n",
      "Paso 2: StringIndexer\n",
      "handleInvalid: how to handle invalid data (unseen or NULL values) in features and label column of string type. Options are 'skip' (filter out rows with invalid data), error (throw an error), or 'keep' (put invalid data in a special additional bucket, at index numLabels). (default: error, current: keep)\n",
      "inputCol: input column name. (current: motivo_prestamo)\n",
      "inputCols: input column names. (undefined)\n",
      "outputCol: output column name. (default: StringIndexer_86a0018c1a2f__output, current: motivo_prestamo_idx)\n",
      "outputCols: output column names. (undefined)\n",
      "stringOrderType: How to order labels of string column. The first label after ordering is assigned an index of 0. Supported options: frequencyDesc, frequencyAsc, alphabetDesc, alphabetAsc. Default is frequencyDesc. In case of equal frequency when under frequencyDesc/Asc, the strings are further sorted alphabetically (default: frequencyDesc)\n",
      "--------------------------------------------------\n",
      "Paso 3: StringIndexer\n",
      "handleInvalid: how to handle invalid data (unseen or NULL values) in features and label column of string type. Options are 'skip' (filter out rows with invalid data), error (throw an error), or 'keep' (put invalid data in a special additional bucket, at index numLabels). (default: error, current: keep)\n",
      "inputCol: input column name. (current: tipo_vivienda)\n",
      "inputCols: input column names. (undefined)\n",
      "outputCol: output column name. (default: StringIndexer_8ae65c497324__output, current: tipo_vivienda_idx)\n",
      "outputCols: output column names. (undefined)\n",
      "stringOrderType: How to order labels of string column. The first label after ordering is assigned an index of 0. Supported options: frequencyDesc, frequencyAsc, alphabetDesc, alphabetAsc. Default is frequencyDesc. In case of equal frequency when under frequencyDesc/Asc, the strings are further sorted alphabetically (default: frequencyDesc)\n",
      "--------------------------------------------------\n",
      "Paso 4: OneHotEncoder\n",
      "dropLast: whether to drop the last category (default: True)\n",
      "handleInvalid: How to handle invalid data during transform(). Options are 'keep' (invalid data presented as an extra categorical feature) or error (throw an error). Note that this Param is only used during transform; during fitting, invalid data will result in an error. (default: error)\n",
      "inputCol: input column name. (undefined)\n",
      "inputCols: input column names. (current: ['motivo_prestamo_idx', 'tipo_vivienda_idx'])\n",
      "outputCol: output column name. (default: OneHotEncoder_fe8b26c07bd7__output)\n",
      "outputCols: output column names. (current: ['motivo_prestamo_oh', 'tipo_vivienda_oh'])\n",
      "--------------------------------------------------\n",
      "Paso 5: VectorAssembler\n",
      "handleInvalid: How to handle invalid data (NULL and NaN values). Options are 'skip' (filter out rows with invalid data), 'error' (throw an error), or 'keep' (return relevant number of NaN in the output). Column lengths are taken from the size of ML Attribute Group, which can be set using `VectorSizeHint` in a pipeline before `VectorAssembler`. Column lengths can also be inferred from first rows of the data since it is safe to do so but only in case of 'error' or 'skip'). (default: error, current: keep)\n",
      "inputCols: input column names. (current: ['monto_aprobado_imp', 'tasa_interes_imp', 'plazo_meses_imp', 'ingreso_anual_imp', 'antiguedad_laboral_imp', 'estado_verif_ingreso_imp', 'promedio_fico_imp', 'anio_apertura_credito_imp', 'cuentas_hipotecarias_imp', 'total_cuentas_credito_imp', 'cuentas_tarjeta_credito_imp', 'saldo_revolvente_imp', 'uso_credito_revolvente_imp', 'limite_credito_total_imp', 'dti_imp', 'meses_ultima_consulta_imp', 'meses_tarjeta_nueva_imp', 'lineas_credito_12m_imp', 'porcentaje_sin_moras_imp', 'moras_2_year_imp', 'monto_total_cobranzas_imp', 'tuvo_acuerdo_pago_imp', 'subcategoria_credito_imp', 'anio_emision_prestamo_imp', 'mes_emision_prestamo_imp', 'motivo_prestamo_oh', 'tipo_vivienda_oh'])\n",
      "outputCol: output column name. (default: VectorAssembler_7e60c6b7bbaa__output, current: features_raw)\n",
      "--------------------------------------------------\n",
      "Paso 6: StandardScaler\n",
      "inputCol: input column name. (current: features_raw)\n",
      "outputCol: output column name. (default: StandardScaler_d2bc1e77e881__output, current: features)\n",
      "withMean: Center data with mean (default: False, current: True)\n",
      "withStd: Scale to unit standard deviation (default: True, current: True)\n",
      "--------------------------------------------------\n",
      "Ajustando pipeline de preprocesamiento...\n",
      "[INFO] DimensiÃ³n de entrada = 45\n"
     ]
    }
   ],
   "source": [
    "# ğŸ”¹ Pipeline de preprocesamiento\n",
    "preprocessing_stages = [imputer] + indexers + [encoder, assembler, scaler]\n",
    "preprocessing_pipeline = Pipeline(stages=preprocessing_stages)\n",
    "\n",
    "for i, stage in enumerate(preprocessing_pipeline.getStages()):\n",
    "    print(f\"Paso {i+1}: {stage.__class__.__name__}\")\n",
    "    print(stage.explainParams())\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# Ajustar el pipeline de preprocesamiento\n",
    "print(\"Ajustando pipeline de preprocesamiento...\")\n",
    "preprocessing_model = preprocessing_pipeline.fit(df)\n",
    "\n",
    "# Transformar todos los datos\n",
    "df_transformed = preprocessing_model.transform(df)\n",
    "\n",
    "# Obtener dimensiÃ³n de entrada\n",
    "sample_vec = (df_transformed.limit(1)\n",
    "              .select(F.col(\"features\").alias(\"sample_features\"))\n",
    "              .head()[\"sample_features\"])\n",
    "input_dim = sample_vec.size\n",
    "print(f\"[INFO] DimensiÃ³n de entrada = {input_dim}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7MA13EVC0ZT_",
    "outputId": "0813be90-9865-41f9-ab43-9d7ac4a03776",
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 107284\n",
      "Test size: 26948\n",
      "ProporciÃ³n de clases en train:\n",
      "+-----+-----+\n",
      "|label|count|\n",
      "+-----+-----+\n",
      "|  0.0|85757|\n",
      "|  1.0|21527|\n",
      "+-----+-----+\n",
      "\n",
      "ProporciÃ³n de clases en test:\n",
      "+-----+-----+\n",
      "|label|count|\n",
      "+-----+-----+\n",
      "|  0.0|21573|\n",
      "|  1.0| 5375|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ğŸ”¹ Split train-test\n",
    "train_df, test_df = df_transformed.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "print(f\"Train size: {train_df.count()}\")\n",
    "print(f\"Test size: {test_df.count()}\")\n",
    "print(\"ProporciÃ³n de clases en train:\")\n",
    "train_df.groupBy('label').count().show()\n",
    "print(\"ProporciÃ³n de clases en test:\")\n",
    "test_df.groupBy('label').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* DefiniciÃ³n del modelo e hiperparÃ¡metros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jli8HbWL0bdj"
   },
   "outputs": [],
   "source": [
    "# ğŸ”¹ Configurar MLP con Grid Search\n",
    "mlp = MultilayerPerceptronClassifier(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"label\",\n",
    "    predictionCol=\"prediction\",\n",
    "    rawPredictionCol=\"rawPrediction\",\n",
    "    maxIter=100,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Definir arquitecturas de red\n",
    "layers_options = [\n",
    "    [input_dim, 10, 2],           # Simple\n",
    "    [input_dim, 50, 2],           # Media\n",
    "    [input_dim, 100, 2],          # Grande\n",
    "]\n",
    "\n",
    "# Definir la malla de parÃ¡metros\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(mlp.layers, layers_options) \\\n",
    "    .addGrid(mlp.stepSize, [0.1, 0.01, 0.001])\\\n",
    "    .build()\n",
    "\n",
    "# Configurar evaluador\n",
    "evaluator = BinaryClassificationEvaluator(\n",
    "    labelCol=\"label\",\n",
    "    rawPredictionCol=\"rawPrediction\",\n",
    "    metricName=\"areaUnderPR\"  # Usamos PR por desbalance de datos\n",
    ")\n",
    "\n",
    "# Configurar CrossValidator\n",
    "crossval = CrossValidator(\n",
    "    estimator=mlp,\n",
    "    estimatorParamMaps=paramGrid,\n",
    "    evaluator=evaluator,\n",
    "    numFolds=3,\n",
    "    seed=42,\n",
    "    collectSubModels=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8cKeeohz0dbe",
    "outputId": "8a6e07c0-0da9-4cac-fc33-2ca34f1ebf7f",
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando grid search...\n",
      "Tiempo de entrenamiento: 3738.77 segundos\n"
     ]
    }
   ],
   "source": [
    "# ğŸ”¹ Entrenar el modelo\n",
    "print(\"Iniciando grid search...\")\n",
    "start_time = time.time()\n",
    "\n",
    "cv_model = crossval.fit(train_df)\n",
    "\n",
    "fit_time = time.time() - start_time\n",
    "print(f\"Tiempo de entrenamiento: {fit_time:.2f} segundos\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YGhCx_FjFFij",
    "outputId": "ea3459f3-78cf-4c49-e29a-9598ec447272",
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "MEJORES PARÃMETROS ENCONTRADOS\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "layers         : [45, 10, 2]\n",
      "stepSize       : 0.1\n",
      "maxIter        : 100\n",
      "blockSize      : 128\n",
      "solver         : l-bfgs\n",
      "\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "OTROS PARÃMETROS\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "featuresCol    : features\n",
      "labelCol       : label\n",
      "predictionCol  : prediction\n",
      "probabilityCol : probability\n",
      "rawPredictionCol: rawPrediction\n",
      "seed           : 42\n",
      "tol            : 1e-06\n"
     ]
    }
   ],
   "source": [
    "# Visualizar los parÃ¡metros\n",
    "best_params = cv_model.bestModel.extractParamMap()\n",
    "\n",
    "print(\"â•\" * 50)\n",
    "print(\"MEJORES PARÃMETROS ENCONTRADOS\")\n",
    "print(\"â•\" * 50)\n",
    "\n",
    "# ParÃ¡metros mÃ¡s importantes primero\n",
    "important_params = ['layers', 'stepSize', 'maxIter', 'blockSize', 'solver']\n",
    "for param_name in important_params:\n",
    "    for param_obj, value in best_params.items():\n",
    "        if param_obj.name == param_name:\n",
    "            print(f\"{param_name:15}: {value}\")\n",
    "            break\n",
    "\n",
    "print(\"\\n\" + \"â•\" * 50)\n",
    "print(\"OTROS PARÃMETROS\")\n",
    "print(\"â•\" * 50)\n",
    "\n",
    "# El resto de parÃ¡metros\n",
    "for param_obj, value in best_params.items():\n",
    "    if param_obj.name not in important_params:\n",
    "        print(f\"{param_obj.name:15}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pUnb8dZqDVja",
    "outputId": "8640ed33-2045-4a2e-d6ff-32fc38acb11b",
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluando modelo con mÃ©tricas\n",
      "\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "RENDIMIENTO DEL MEJOR MODELO - MÃ‰TRICAS COMPLETAS\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "MÃ©trica         Train      Test       Diferencia  \n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "AUC_ROC         0.7504    0.7389    0.0115\n",
      "AUC_PR          0.5129    0.4935    0.0195\n",
      "F1              0.7834    0.7804    0.0030\n",
      "Precision       0.8194    0.8123    0.0071\n",
      "Recall          0.8274    0.8251    0.0023\n",
      "Accuracy        0.8274    0.8251    0.0023\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n"
     ]
    }
   ],
   "source": [
    "# 7. Evaluar el mejor modelo COMPLETAMENTE\n",
    "print(\"Evaluando modelo con mÃ©tricas\")\n",
    "best_model = cv_model.bestModel\n",
    "\n",
    "# EvaluaciÃ³n comprehensiva\n",
    "results = evaluate_model_comprehensive(best_model, train_df, test_df)\n",
    "\n",
    "# Mostrar resultados en formato tabular\n",
    "print(\"\\n\" + \"â•\" * 80)\n",
    "print(\"RENDIMIENTO DEL MEJOR MODELO - MÃ‰TRICAS COMPLETAS\")\n",
    "print(\"â•\" * 80)\n",
    "print(f\"{'MÃ©trica':<15} {'Train':<10} {'Test':<10} {'Diferencia':<12}\")\n",
    "print(\"â”€\" * 80)\n",
    "\n",
    "metrics_to_display = ['AUC_ROC', 'AUC_PR', 'F1', 'Precision', 'Recall', 'Accuracy']\n",
    "\n",
    "for metric in metrics_to_display:\n",
    "    train_val = results['train_metrics'][metric]\n",
    "    test_val = results['test_metrics'][metric]\n",
    "    difference = abs(train_val - test_val)\n",
    "\n",
    "    print(f\"{metric:<15} {train_val:.4f}    {test_val:.4f}    {difference:.4f}\")\n",
    "\n",
    "print(\"â•\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R2AkQPwIDZfp",
    "outputId": "89d59253-1267-48bf-df39-5bb2125ba321",
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MATRIZ DE CONFUSIÃ“N\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "Actual \\ Predicted |  0  |  1  |\n",
      "-------------------|-----|-----|\n",
      "        0          |21224| 349 |\n",
      "-------------------|-----|-----|\n",
      "        1          |4364 |1011 |\n",
      "-------------------|-----|-----|\n"
     ]
    }
   ],
   "source": [
    "test_predictions = best_model.transform(test_df)\n",
    "def get_confusion_matrix_robust(predictions, labelCol=\"label\", predictionCol=\"prediction\"):\n",
    "    \"\"\"VersiÃ³n robusta para obtener matriz de confusiÃ³n\"\"\"\n",
    "\n",
    "    # Calcular conteos\n",
    "    confusion_counts = predictions.groupBy(\n",
    "        F.col(labelCol).alias(\"actual\"),\n",
    "        F.col(predictionCol).alias(\"predicted\")\n",
    "    ).count()\n",
    "\n",
    "    # Colectar datos de forma segura\n",
    "    confusion_data = []\n",
    "    for row in confusion_counts.collect():\n",
    "        confusion_data.append({\n",
    "            'actual': row['actual'],\n",
    "            'predicted': row['predicted'],\n",
    "            'count': row['count']\n",
    "        })\n",
    "\n",
    "    # Crear matriz 2x2\n",
    "    matrix = {\n",
    "        (0.0, 0.0): 0, (0.0, 1.0): 0,\n",
    "        (1.0, 0.0): 0, (1.0, 1.0): 0\n",
    "    }\n",
    "\n",
    "    for item in confusion_data:\n",
    "        matrix[(item['actual'], item['predicted'])] = item['count']\n",
    "\n",
    "    return matrix\n",
    "\n",
    "# Usar la funciÃ³n robusta\n",
    "confusion_matrix = get_confusion_matrix_robust(test_predictions)\n",
    "\n",
    "print(\"MATRIZ DE CONFUSIÃ“N\")\n",
    "print(\"â•\" * 40)\n",
    "print(\"Actual \\\\ Predicted |  0  |  1  |\")\n",
    "print(\"-------------------|-----|-----|\")\n",
    "print(f\"        0          |{confusion_matrix[(0.0, 0.0)]:4d}|{confusion_matrix[(0.0, 1.0)]:4d} |\")\n",
    "print(\"-------------------|-----|-----|\")\n",
    "print(f\"        1          |{confusion_matrix[(1.0, 0.0)]:4d} |{confusion_matrix[(1.0, 1.0)]:4d} |\")\n",
    "print(\"-------------------|-----|-----|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R1JyV7-lDcaF",
    "outputId": "0aff82e4-8d64-4344-c2dc-0cf0ee18365c",
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MÃ‰TRICAS CALCULADAS DIRECTAMENTE DESDE SPARK:\n",
      " class_0: Precision=0.8295, Recall=0.9838, F1=0.9001\n",
      " class_1: Precision=0.7434, Recall=0.1881, F1=0.3002\n",
      "\n",
      " Matriz de ConfusiÃ³n:\n",
      "True Negatives (TN):  21224\n",
      "False Positives (FP): 349\n",
      "False Negatives (FN): 4364\n",
      "True Positives (TP):  1011\n"
     ]
    }
   ],
   "source": [
    "def calculate_metrics_from_spark(test_predictions, labelCol=\"label\", predictionCol=\"prediction\"):\n",
    "    \"\"\"Calcular mÃ©tricas directamente desde DataFrame de Spark\"\"\"\n",
    "\n",
    "    # Calcular matriz de confusiÃ³n manualmente\n",
    "    confusion_data = test_predictions.groupBy(\n",
    "        F.col(labelCol).alias(\"Actual\"),\n",
    "        F.col(predictionCol).alias(\"Predicted\")\n",
    "    ).count().collect()\n",
    "\n",
    "    # Inicializar contadores\n",
    "    tn, fp, fn, tp = 0, 0, 0, 0\n",
    "\n",
    "    for row in confusion_data:\n",
    "        if row['Actual'] == 0.0 and row['Predicted'] == 0.0:\n",
    "            tn = row['count']\n",
    "        elif row['Actual'] == 0.0 and row['Predicted'] == 1.0:\n",
    "            fp = row['count']\n",
    "        elif row['Actual'] == 1.0 and row['Predicted'] == 0.0:\n",
    "            fn = row['count']\n",
    "        elif row['Actual'] == 1.0 and row['Predicted'] == 1.0:\n",
    "            tp = row['count']\n",
    "\n",
    "    # Calcular mÃ©tricas\n",
    "    precision_0 = tn / (tn + fn) if (tn + fn) > 0 else 0\n",
    "    recall_0 = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "    f1_0 = 2 * (precision_0 * recall_0) / (precision_0 + recall_0) if (precision_0 + recall_0) > 0 else 0\n",
    "\n",
    "    precision_1 = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall_1 = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1_1 = 2 * (precision_1 * recall_1) / (precision_1 + recall_1) if (precision_1 + recall_1) > 0 else 0\n",
    "\n",
    "    return {\n",
    "        'class_0': {'precision': precision_0, 'recall': recall_0, 'f1': f1_0},\n",
    "        'class_1': {'precision': precision_1, 'recall': recall_1, 'f1': f1_1},\n",
    "        'confusion_matrix': {'TN': tn, 'FP': fp, 'FN': fn, 'TP': tp}\n",
    "    }\n",
    "\n",
    "# Usar esta versiÃ³n alternativa\n",
    "spark_metrics = calculate_metrics_from_spark(test_predictions)\n",
    "\n",
    "print(\"MÃ‰TRICAS CALCULADAS DIRECTAMENTE DESDE SPARK:\")\n",
    "for class_name, metrics in spark_metrics.items():\n",
    "    if class_name != 'confusion_matrix':\n",
    "        print(f\" {class_name}: Precision={metrics['precision']:.4f}, Recall={metrics['recall']:.4f}, F1={metrics['f1']:.4f}\")\n",
    "\n",
    "# Mostrar matriz de confusiÃ³n tambiÃ©n\n",
    "print(f\"\\n Matriz de ConfusiÃ³n:\")\n",
    "print(f\"True Negatives (TN):  {spark_metrics['confusion_matrix']['TN']}\")\n",
    "print(f\"False Positives (FP): {spark_metrics['confusion_matrix']['FP']}\")\n",
    "print(f\"False Negatives (FN): {spark_metrics['confusion_matrix']['FN']}\")\n",
    "print(f\"True Positives (TP):  {spark_metrics['confusion_matrix']['TP']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "laQN7u3_Dvx2",
    "outputId": "60e5a77a-2132-4a93-eef9-fab60b0c7e6a",
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ANÃLISIS DE OVERFITTING:\n",
      "âœ… AUC_ROC: Buen generalizaciÃ³n (Diff = 0.0115)\n",
      "âœ… AUC_PR: Buen generalizaciÃ³n (Diff = 0.0195)\n",
      "âœ… F1: Buen generalizaciÃ³n (Diff = 0.0030)\n",
      "âœ… Precision: Buen generalizaciÃ³n (Diff = 0.0071)\n",
      "âœ… Recall: Buen generalizaciÃ³n (Diff = 0.0023)\n",
      "âœ… Accuracy: Buen generalizaciÃ³n (Diff = 0.0023)\n"
     ]
    }
   ],
   "source": [
    "# AnÃ¡lisis de overfitting\n",
    "print(\"\\n ANÃLISIS DE OVERFITTING:\")\n",
    "overfitting_threshold = 0.05  # Diferencia mÃ¡xima aceptable\n",
    "\n",
    "for metric in metrics_to_display:\n",
    "    train_val = results['train_metrics'][metric]\n",
    "    test_val = results['test_metrics'][metric]\n",
    "    difference = abs(train_val - test_val)\n",
    "\n",
    "    if difference > overfitting_threshold:\n",
    "        print(f\" Posible overfitting en {metric}: Diff = {difference:.4f}\")\n",
    "    else:\n",
    "        print(f\"âœ… {metric}: Buen generalizaciÃ³n (Diff = {difference:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los resultados del grid search indican que se ha encontrado un modelo con buen rendimiento y excelente capacidad de generalizaciÃ³n. La estructura de red neuronal [45, 10, 2] con el optimizador L-BFGS y parÃ¡metros especÃ­ficos (stepSize: 0.1, maxIter: 100) logra un equilibrio Ã³ptimo entre aprendizaje y generalizaciÃ³n, como evidencia la mÃ­nima diferencia entre las mÃ©tricas de entrenamiento y prueba (todas por debajo del 2%). \n",
    "\n",
    "El AUC-ROC de 0.7389 en test demuestra una capacidad aceptable de discriminaciÃ³n entre clases, mientras que el accuracy del 82.51% refleja una precisiÃ³n general sÃ³lida del modelo.\n",
    "\n",
    "Sin embargo, el anÃ¡lisis detallado revela un desbalance significativo en el rendimiento por clases. Mientras la clase 0 muestra excelentes mÃ©tricas (Precision: 0.8295, Recall: 0.9838), la clase 1 presenta un recall muy bajo (0.1881), indicando que el modelo identifica correctamente solo el 18.81% de los casos positivos reales. \n",
    "\n",
    "Esta disparidad, evidente en la matriz de confusiÃ³n con 4364 falsos negativos frente a solo 1011 verdaderos positivos, sugiere que el modelo tiene tendencia a predecir la clase mayoritaria."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K6hS15Nu0Rri"
   },
   "source": [
    "__________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3WhkF7ec69L6"
   },
   "source": [
    "## MLP con los mejores hiperparÃ¡metros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Uso de todos los registros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cpo2JKeK7JqZ"
   },
   "outputs": [],
   "source": [
    "# ğŸ”¹ Lectura de datos [100% de los registros]\n",
    "full_data = spark.read.csv(\"data_to_model.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-SuTr7yO7bkm",
    "outputId": "3420afcc-a4cf-481c-8cf1-de70b93c879f",
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variables categÃ³ricas: ['motivo_prestamo', 'tipo_vivienda']\n",
      "Variables numÃ©ricas: ['monto_aprobado', 'tasa_interes', 'plazo_meses', 'ingreso_anual', 'antiguedad_laboral', 'estado_verif_ingreso', 'promedio_fico', 'anio_apertura_credito', 'cuentas_hipotecarias', 'total_cuentas_credito', 'cuentas_tarjeta_credito', 'saldo_revolvente', 'uso_credito_revolvente', 'limite_credito_total', 'dti', 'meses_ultima_consulta', 'meses_tarjeta_nueva', 'lineas_credito_12m', 'porcentaje_sin_moras', 'moras_2_year', 'monto_total_cobranzas', 'tuvo_acuerdo_pago', 'subcategoria_credito', 'anio_emision_prestamo', 'mes_emision_prestamo']\n"
     ]
    }
   ],
   "source": [
    "# ğŸ”¹ ConfiguraciÃ³n inicial\n",
    "\n",
    "# Variable objetivo\n",
    "label_col = \"default\"\n",
    "\n",
    "# DetecciÃ³n automÃ¡tica de columnas\n",
    "categorical_cols = [\n",
    "    col for col in full_data.columns\n",
    "    if full_data.schema[col].dataType.typeName() == 'string' and col != label_col]\n",
    "numeric_cols = [\n",
    "    col for col in full_data.columns\n",
    "    if full_data.schema[col].dataType.typeName() in ['integer', 'double', 'float'] and col != label_col]\n",
    "\n",
    "print(f\"Variables categÃ³ricas: {categorical_cols}\")\n",
    "print(f\"Variables numÃ©ricas: {numeric_cols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GuLqdPDu72AD",
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# ğŸ”¹ Preprocesamiento\n",
    "\n",
    "# Convertir label a DoubleType y filtrar nulos\n",
    "full_data = full_data.withColumn(\"label\", F.col(label_col).cast(DoubleType()))\\\n",
    "       .filter(F.col(\"label\").isNotNull())\n",
    "\n",
    "# ImputaciÃ³n de valores nulos en variables numÃ©ricas\n",
    "imputer = Imputer(\n",
    "    inputCols=numeric_cols,\n",
    "    outputCols=[f\"{c}_imp\" for c in numeric_cols],\n",
    "    strategy=\"median\"\n",
    ")\n",
    "num_imp_cols = [f\"{c}_imp\" for c in numeric_cols]\n",
    "\n",
    "# Indexado y encoding de variables categÃ³ricas\n",
    "indexers = [StringIndexer(inputCol=c, outputCol=f\"{c}_idx\", handleInvalid=\"keep\")\n",
    "            for c in categorical_cols]\n",
    "\n",
    "encoder = OneHotEncoder(\n",
    "    inputCols=[f\"{c}_idx\" for c in categorical_cols],\n",
    "    outputCols=[f\"{c}_oh\" for c in categorical_cols]\n",
    ")\n",
    "ohe_cols = [f\"{c}_oh\" for c in categorical_cols]\n",
    "\n",
    "# Assembler y scaler\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=num_imp_cols + ohe_cols,\n",
    "    outputCol=\"features_raw\",\n",
    "    handleInvalid=\"keep\"\n",
    ")\n",
    "\n",
    "scaler = StandardScaler(\n",
    "    inputCol=\"features_raw\",\n",
    "    outputCol=\"features\",\n",
    "    withStd=True,\n",
    "    withMean=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mtI0-Ozh9IV7",
    "outputId": "25071bc0-9248-46af-9317-49308077fb9f",
    "tags": [
     "remove-input",
     "hide-output"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paso 1: Imputer\n",
      "inputCol: input column name. (undefined)\n",
      "inputCols: input column names. (current: ['monto_aprobado', 'tasa_interes', 'plazo_meses', 'ingreso_anual', 'antiguedad_laboral', 'estado_verif_ingreso', 'promedio_fico', 'anio_apertura_credito', 'cuentas_hipotecarias', 'total_cuentas_credito', 'cuentas_tarjeta_credito', 'saldo_revolvente', 'uso_credito_revolvente', 'limite_credito_total', 'dti', 'meses_ultima_consulta', 'meses_tarjeta_nueva', 'lineas_credito_12m', 'porcentaje_sin_moras', 'moras_2_year', 'monto_total_cobranzas', 'tuvo_acuerdo_pago', 'subcategoria_credito', 'anio_emision_prestamo', 'mes_emision_prestamo'])\n",
      "missingValue: The placeholder for the missing values. All occurrences of missingValue will be imputed. (default: nan)\n",
      "outputCol: output column name. (default: Imputer_b4c2aa4cccf0__output)\n",
      "outputCols: output column names. (current: ['monto_aprobado_imp', 'tasa_interes_imp', 'plazo_meses_imp', 'ingreso_anual_imp', 'antiguedad_laboral_imp', 'estado_verif_ingreso_imp', 'promedio_fico_imp', 'anio_apertura_credito_imp', 'cuentas_hipotecarias_imp', 'total_cuentas_credito_imp', 'cuentas_tarjeta_credito_imp', 'saldo_revolvente_imp', 'uso_credito_revolvente_imp', 'limite_credito_total_imp', 'dti_imp', 'meses_ultima_consulta_imp', 'meses_tarjeta_nueva_imp', 'lineas_credito_12m_imp', 'porcentaje_sin_moras_imp', 'moras_2_year_imp', 'monto_total_cobranzas_imp', 'tuvo_acuerdo_pago_imp', 'subcategoria_credito_imp', 'anio_emision_prestamo_imp', 'mes_emision_prestamo_imp'])\n",
      "relativeError: the relative target precision for the approximate quantile algorithm. Must be in the range [0, 1] (default: 0.001)\n",
      "strategy: strategy for imputation. If mean, then replace missing values using the mean value of the feature. If median, then replace missing values using the median value of the feature. If mode, then replace missing using the most frequent value of the feature. (default: mean, current: median)\n",
      "--------------------------------------------------\n",
      "Paso 2: StringIndexer\n",
      "handleInvalid: how to handle invalid data (unseen or NULL values) in features and label column of string type. Options are 'skip' (filter out rows with invalid data), error (throw an error), or 'keep' (put invalid data in a special additional bucket, at index numLabels). (default: error, current: keep)\n",
      "inputCol: input column name. (current: motivo_prestamo)\n",
      "inputCols: input column names. (undefined)\n",
      "outputCol: output column name. (default: StringIndexer_dac6857c81bf__output, current: motivo_prestamo_idx)\n",
      "outputCols: output column names. (undefined)\n",
      "stringOrderType: How to order labels of string column. The first label after ordering is assigned an index of 0. Supported options: frequencyDesc, frequencyAsc, alphabetDesc, alphabetAsc. Default is frequencyDesc. In case of equal frequency when under frequencyDesc/Asc, the strings are further sorted alphabetically (default: frequencyDesc)\n",
      "--------------------------------------------------\n",
      "Paso 3: StringIndexer\n",
      "handleInvalid: how to handle invalid data (unseen or NULL values) in features and label column of string type. Options are 'skip' (filter out rows with invalid data), error (throw an error), or 'keep' (put invalid data in a special additional bucket, at index numLabels). (default: error, current: keep)\n",
      "inputCol: input column name. (current: tipo_vivienda)\n",
      "inputCols: input column names. (undefined)\n",
      "outputCol: output column name. (default: StringIndexer_8ebb3279606d__output, current: tipo_vivienda_idx)\n",
      "outputCols: output column names. (undefined)\n",
      "stringOrderType: How to order labels of string column. The first label after ordering is assigned an index of 0. Supported options: frequencyDesc, frequencyAsc, alphabetDesc, alphabetAsc. Default is frequencyDesc. In case of equal frequency when under frequencyDesc/Asc, the strings are further sorted alphabetically (default: frequencyDesc)\n",
      "--------------------------------------------------\n",
      "Paso 4: OneHotEncoder\n",
      "dropLast: whether to drop the last category (default: True)\n",
      "handleInvalid: How to handle invalid data during transform(). Options are 'keep' (invalid data presented as an extra categorical feature) or error (throw an error). Note that this Param is only used during transform; during fitting, invalid data will result in an error. (default: error)\n",
      "inputCol: input column name. (undefined)\n",
      "inputCols: input column names. (current: ['motivo_prestamo_idx', 'tipo_vivienda_idx'])\n",
      "outputCol: output column name. (default: OneHotEncoder_86f37fb340dd__output)\n",
      "outputCols: output column names. (current: ['motivo_prestamo_oh', 'tipo_vivienda_oh'])\n",
      "--------------------------------------------------\n",
      "Paso 5: VectorAssembler\n",
      "handleInvalid: How to handle invalid data (NULL and NaN values). Options are 'skip' (filter out rows with invalid data), 'error' (throw an error), or 'keep' (return relevant number of NaN in the output). Column lengths are taken from the size of ML Attribute Group, which can be set using `VectorSizeHint` in a pipeline before `VectorAssembler`. Column lengths can also be inferred from first rows of the data since it is safe to do so but only in case of 'error' or 'skip'). (default: error, current: keep)\n",
      "inputCols: input column names. (current: ['monto_aprobado_imp', 'tasa_interes_imp', 'plazo_meses_imp', 'ingreso_anual_imp', 'antiguedad_laboral_imp', 'estado_verif_ingreso_imp', 'promedio_fico_imp', 'anio_apertura_credito_imp', 'cuentas_hipotecarias_imp', 'total_cuentas_credito_imp', 'cuentas_tarjeta_credito_imp', 'saldo_revolvente_imp', 'uso_credito_revolvente_imp', 'limite_credito_total_imp', 'dti_imp', 'meses_ultima_consulta_imp', 'meses_tarjeta_nueva_imp', 'lineas_credito_12m_imp', 'porcentaje_sin_moras_imp', 'moras_2_year_imp', 'monto_total_cobranzas_imp', 'tuvo_acuerdo_pago_imp', 'subcategoria_credito_imp', 'anio_emision_prestamo_imp', 'mes_emision_prestamo_imp', 'motivo_prestamo_oh', 'tipo_vivienda_oh'])\n",
      "outputCol: output column name. (default: VectorAssembler_230d91db8e0a__output, current: features_raw)\n",
      "--------------------------------------------------\n",
      "Paso 6: StandardScaler\n",
      "inputCol: input column name. (current: features_raw)\n",
      "outputCol: output column name. (default: StandardScaler_8399a487e431__output, current: features)\n",
      "withMean: Center data with mean (default: False, current: True)\n",
      "withStd: Scale to unit standard deviation (default: True, current: True)\n",
      "--------------------------------------------------\n",
      "Ajustando pipeline de preprocesamiento...\n",
      "DimensiÃ³n de entrada = 45\n"
     ]
    }
   ],
   "source": [
    "# ğŸ”¹ Pipeline de preprocesamiento\n",
    "preprocessing_stages = [imputer] + indexers + [encoder, assembler, scaler]\n",
    "preprocessing_pipeline = Pipeline(stages=preprocessing_stages)\n",
    "\n",
    "for i, stage in enumerate(preprocessing_pipeline.getStages()):\n",
    "    print(f\"Paso {i+1}: {stage.__class__.__name__}\")\n",
    "    print(stage.explainParams())\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# Ajustar el pipeline de preprocesamiento\n",
    "print(\"Ajustando pipeline de preprocesamiento...\")\n",
    "preprocessing_model = preprocessing_pipeline.fit(full_data)\n",
    "\n",
    "# Transformar todos los datos\n",
    "full_data_transformed = preprocessing_model.transform(full_data)\n",
    "\n",
    "# Obtener dimensiÃ³n de entrada\n",
    "sample_vec = (df_transformed.limit(1)\n",
    "              .select(F.col(\"features\").alias(\"sample_features\"))\n",
    "              .head()[\"sample_features\"])\n",
    "input_dim = sample_vec.size\n",
    "print(f\"DimensiÃ³n de entrada = {input_dim}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5DGIIbLu9YKh",
    "outputId": "737b1ec1-b457-44d9-cbc9-bc85eaea8618",
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 1076425\n",
      "Test size: 268885\n",
      "ProporciÃ³n de clases en train:\n",
      "+-----+------+\n",
      "|label| count|\n",
      "+-----+------+\n",
      "|  0.0|861332|\n",
      "|  1.0|215093|\n",
      "+-----+------+\n",
      "\n",
      "ProporciÃ³n de clases en test:\n",
      "+-----+------+\n",
      "|label| count|\n",
      "+-----+------+\n",
      "|  0.0|215419|\n",
      "|  1.0| 53466|\n",
      "+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ğŸ”¹ Split train-test\n",
    "train_data, test_data = full_data_transformed.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "print(f\"Train size: {train_data.count()}\")\n",
    "print(f\"Test size: {test_data.count()}\")\n",
    "print(\"ProporciÃ³n de clases en train:\")\n",
    "train_data.groupBy('label').count().show()\n",
    "print(\"ProporciÃ³n de clases en test:\")\n",
    "test_data.groupBy('label').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Modelo MLP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "hQhBia9R32Zr"
   },
   "outputs": [],
   "source": [
    "# ğŸ”¹ Configurar modelo con los mejores hiperparÃ¡metros\n",
    "final_mlp = MultilayerPerceptronClassifier(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"label\",\n",
    "    predictionCol=\"prediction\",\n",
    "    rawPredictionCol=\"rawPrediction\",\n",
    "    layers=[45, 10, 2],      # Mejores parÃ¡metros\n",
    "    stepSize=0.01,           # Learning rate Ã³ptimo\n",
    "    maxIter=100,\n",
    "    blockSize=128,           # Block size para mÃ¡s datos\n",
    "    solver=\"l-bfgs\",         # Algoritmo eficiente\n",
    "    tol=1e-06,\n",
    "    seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CiX8P-cj1pEV",
    "outputId": "3cf28dde-0a88-4e42-a770-700af4f09471",
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrenando modelo final-----\n",
      "Entrenamiento completado en 5.36 minutos\n"
     ]
    }
   ],
   "source": [
    "# ğŸ”¹ Entrenar modelo final\n",
    "print(\"Entrenando modelo final-----\")\n",
    "start_time = time.time()\n",
    "\n",
    "final_model = final_mlp.fit(train_data)\n",
    "\n",
    "training_time = (time.time() - start_time) / 60\n",
    "print(f\"Entrenamiento completado en {training_time:.2f} minutos\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lKnVHNFKHYg3",
    "outputId": "e81728c4-d625-48d6-bdfc-bf93e315422d",
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluando modelo final con mÃ©tricas\n",
      "Tiempo de evaluaciÃ³n: 0.00 minutos\n",
      "\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "RENDIMIENTO DEL MEJOR MODELO - MÃ‰TRICAS COMPLETAS\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "MÃ©trica         Train      Test       Diferencia  \n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "AUC_ROC         0.7426    0.7440    0.0014\n",
      "AUC_PR          0.5004    0.5026    0.0022\n",
      "F1              0.7808    0.7829    0.0020\n",
      "Precision       0.8232    0.8243    0.0011\n",
      "Recall          0.8275    0.8288    0.0013\n",
      "Accuracy        0.8275    0.8288    0.0013\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n"
     ]
    }
   ],
   "source": [
    "# ğŸ”¹ Evaluar el modelo\n",
    "\n",
    "print(\"Evaluando modelo final con mÃ©tricas\")\n",
    "start_time = time.time()\n",
    "test_predictions = final_model.transform(test_data)\n",
    "fit_time = (time.time() - start_time)/60\n",
    "print(f\"Tiempo de evaluaciÃ³n: {fit_time:.2f} minutos\")\n",
    "\n",
    "# EvaluaciÃ³n comprehensiva\n",
    "results = evaluate_model_comprehensive(final_model, train_data, test_data)\n",
    "\n",
    "# Mostrar resultados en formato tabular\n",
    "print(\"\\n\" + \"â•\" * 80)\n",
    "print(\"RENDIMIENTO DEL MEJOR MODELO - MÃ‰TRICAS COMPLETAS\")\n",
    "print(\"â•\" * 80)\n",
    "print(f\"{'MÃ©trica':<15} {'Train':<10} {'Test':<10} {'Diferencia':<12}\")\n",
    "print(\"â”€\" * 80)\n",
    "\n",
    "metrics_to_display = ['AUC_ROC', 'AUC_PR', 'F1', 'Precision', 'Recall', 'Accuracy']\n",
    "\n",
    "for metric in metrics_to_display:\n",
    "    train_val = results['train_metrics'][metric]\n",
    "    test_val = results['test_metrics'][metric]\n",
    "    difference = abs(train_val - test_val)\n",
    "\n",
    "    print(f\"{metric:<15} {train_val:.4f}    {test_val:.4f}    {difference:.4f}\")\n",
    "\n",
    "print(\"â•\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_CoP2h8nJ-gH",
    "outputId": "1845f3d5-c5d5-49ba-ad6f-6d89193d6cb2",
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MATRIZ DE CONFUSIÃ“N\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "Actual \\ Predicted |  0  |  1  |\n",
      "-------------------|------|-----|\n",
      "        0          |212951|2468 |\n",
      "-------------------|------|-----|\n",
      "        1          |43557 |9909 |\n",
      "-------------------|------|-----|\n"
     ]
    }
   ],
   "source": [
    "def get_confusion_matrix_robust(predictions, labelCol=\"label\", predictionCol=\"prediction\"):\n",
    "    \"\"\"VersiÃ³n robusta para obtener matriz de confusiÃ³n\"\"\"\n",
    "\n",
    "    # Calcular conteos\n",
    "    confusion_counts = predictions.groupBy(\n",
    "        F.col(labelCol).alias(\"actual\"),\n",
    "        F.col(predictionCol).alias(\"predicted\")\n",
    "    ).count()\n",
    "\n",
    "    # Colectar datos de forma segura\n",
    "    confusion_data = []\n",
    "    for row in confusion_counts.collect():\n",
    "        confusion_data.append({\n",
    "            'actual': row['actual'],\n",
    "            'predicted': row['predicted'],\n",
    "            'count': row['count']\n",
    "        })\n",
    "\n",
    "    # Crear matriz 2x2\n",
    "    matrix = {\n",
    "        (0.0, 0.0): 0, (0.0, 1.0): 0,\n",
    "        (1.0, 0.0): 0, (1.0, 1.0): 0\n",
    "    }\n",
    "\n",
    "    for item in confusion_data:\n",
    "        matrix[(item['actual'], item['predicted'])] = item['count']\n",
    "\n",
    "    return matrix\n",
    "\n",
    "# Usar la funciÃ³n robusta\n",
    "confusion_matrix = get_confusion_matrix_robust(test_predictions)\n",
    "\n",
    "print(\"MATRIZ DE CONFUSIÃ“N\")\n",
    "print(\"â•\" * 40)\n",
    "print(\"Actual \\\\ Predicted |  0  |  1  |\")\n",
    "print(\"-------------------|------|-----|\")\n",
    "print(f\"        0          |{confusion_matrix[(0.0, 0.0)]:4d}|{confusion_matrix[(0.0, 1.0)]:4d} |\")\n",
    "print(\"-------------------|------|-----|\")\n",
    "print(f\"        1          |{confusion_matrix[(1.0, 0.0)]:4d} |{confusion_matrix[(1.0, 1.0)]:4d} |\")\n",
    "print(\"-------------------|------|-----|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hWyRawy-KEwr",
    "outputId": "61766474-0887-47de-930f-6a9be4fd991f",
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MÃ‰TRICAS CALCULADAS DIRECTAMENTE DESDE SPARK:\n",
      " class_0: Precision=0.8302, Recall=0.9885, F1=0.9025\n",
      " class_1: Precision=0.8006, Recall=0.1853, F1=0.3010\n",
      "\n",
      " Matriz de ConfusiÃ³n:\n",
      "True Negatives (TN):  212951\n",
      "False Positives (FP): 2468\n",
      "False Negatives (FN): 43557\n",
      "True Positives (TP):  9909\n"
     ]
    }
   ],
   "source": [
    "def calculate_metrics_from_spark(test_predictions, labelCol=\"label\", predictionCol=\"prediction\"):\n",
    "    \"\"\"Calcular mÃ©tricas directamente desde DataFrame de Spark\"\"\"\n",
    "\n",
    "    # Calcular matriz de confusiÃ³n manualmente\n",
    "    confusion_data = test_predictions.groupBy(\n",
    "        F.col(labelCol).alias(\"Actual\"),\n",
    "        F.col(predictionCol).alias(\"Predicted\")\n",
    "    ).count().collect()\n",
    "\n",
    "    # Inicializar contadores\n",
    "    tn, fp, fn, tp = 0, 0, 0, 0\n",
    "\n",
    "    for row in confusion_data:\n",
    "        if row['Actual'] == 0.0 and row['Predicted'] == 0.0:\n",
    "            tn = row['count']\n",
    "        elif row['Actual'] == 0.0 and row['Predicted'] == 1.0:\n",
    "            fp = row['count']\n",
    "        elif row['Actual'] == 1.0 and row['Predicted'] == 0.0:\n",
    "            fn = row['count']\n",
    "        elif row['Actual'] == 1.0 and row['Predicted'] == 1.0:\n",
    "            tp = row['count']\n",
    "\n",
    "    # Calcular mÃ©tricas\n",
    "    precision_0 = tn / (tn + fn) if (tn + fn) > 0 else 0\n",
    "    recall_0 = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "    f1_0 = 2 * (precision_0 * recall_0) / (precision_0 + recall_0) if (precision_0 + recall_0) > 0 else 0\n",
    "\n",
    "    precision_1 = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall_1 = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1_1 = 2 * (precision_1 * recall_1) / (precision_1 + recall_1) if (precision_1 + recall_1) > 0 else 0\n",
    "\n",
    "    return {\n",
    "        'class_0': {'precision': precision_0, 'recall': recall_0, 'f1': f1_0},\n",
    "        'class_1': {'precision': precision_1, 'recall': recall_1, 'f1': f1_1},\n",
    "        'confusion_matrix': {'TN': tn, 'FP': fp, 'FN': fn, 'TP': tp}\n",
    "    }\n",
    "\n",
    "# Usar esta versiÃ³n alternativa\n",
    "spark_metrics = calculate_metrics_from_spark(test_predictions)\n",
    "\n",
    "print(\"MÃ‰TRICAS CALCULADAS DIRECTAMENTE DESDE SPARK:\")\n",
    "for class_name, metrics in spark_metrics.items():\n",
    "    if class_name != 'confusion_matrix':\n",
    "        print(f\" {class_name}: Precision={metrics['precision']:.4f}, Recall={metrics['recall']:.4f}, F1={metrics['f1']:.4f}\")\n",
    "\n",
    "# Mostrar matriz de confusiÃ³n tambiÃ©n\n",
    "print(f\"\\n Matriz de ConfusiÃ³n:\")\n",
    "print(f\"True Negatives (TN):  {spark_metrics['confusion_matrix']['TN']}\")\n",
    "print(f\"False Positives (FP): {spark_metrics['confusion_matrix']['FP']}\")\n",
    "print(f\"False Negatives (FN): {spark_metrics['confusion_matrix']['FN']}\")\n",
    "print(f\"True Positives (TP):  {spark_metrics['confusion_matrix']['TP']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XMD5HVUwKM24",
    "outputId": "30f3a029-1bda-481e-9184-77f49e249567",
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ANÃLISIS DE OVERFITTING:\n",
      "âœ… AUC_ROC: Buen generalizaciÃ³n (Diff = 0.0014)\n",
      "âœ… AUC_PR: Buen generalizaciÃ³n (Diff = 0.0022)\n",
      "âœ… F1: Buen generalizaciÃ³n (Diff = 0.0020)\n",
      "âœ… Precision: Buen generalizaciÃ³n (Diff = 0.0011)\n",
      "âœ… Recall: Buen generalizaciÃ³n (Diff = 0.0013)\n",
      "âœ… Accuracy: Buen generalizaciÃ³n (Diff = 0.0013)\n"
     ]
    }
   ],
   "source": [
    "# AnÃ¡lisis de overfitting\n",
    "print(\"\\n ANÃLISIS DE OVERFITTING:\")\n",
    "overfitting_threshold = 0.05  # Diferencia mÃ¡xima aceptable\n",
    "\n",
    "for metric in metrics_to_display:\n",
    "    train_val = results['train_metrics'][metric]\n",
    "    test_val = results['test_metrics'][metric]\n",
    "    difference = abs(train_val - test_val)\n",
    "\n",
    "    if difference > overfitting_threshold:\n",
    "        print(f\" Posible overfitting en {metric}: Diff = {difference:.4f}\")\n",
    "    else:\n",
    "        print(f\"âœ… {metric}: Buen generalizaciÃ³n (Diff = {difference:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El modelo final demuestra un excelente rendimiento en tÃ©rminos de generalizaciÃ³n, con diferencias mÃ­nimas entre las mÃ©tricas de entrenamiento y prueba (todas inferiores al 0,23 %), lo que indica una efectiva prevenciÃ³n del sobreajuste. Un AUC-ROC de 0,7440 y una precisiÃ³n del 82,88 % reflejan una capacidad predictiva sÃ³lida y consistente. \n",
    "\n",
    "Sin embargo, persiste un desafÃ­o crÃ­tico en el desequilibrio de clases: mientras que la clase mayoritaria (0) muestra un rendimiento excepcional con un recall del 98,85 % â€”capturando casi todos sus casos verdaderosâ€”, la clase minoritaria (1) tiene un recall extremadamente bajo del 18,53 %, lo que significa que el modelo solo identifica correctamente menos de una quinta parte de los casos positivos reales.\n",
    "\n",
    "Esta disparidad significativa, que se evidencia en los 43 557 falsos negativos frente a los 9909 verdaderos positivos, indica que el modelo tiende a predecir la clase mayoritaria. Aunque las mÃ©tricas agregadas son sÃ³lidas, el bajo Ã­ndice de recuperaciÃ³n para la clase 1 limita considerablemente la utilidad del modelo en aplicaciones en las que la detecciÃ³n de casos positivos es crucial, como en el diagnÃ³stico mÃ©dico o la detecciÃ³n de fraudes, ya que los falsos negativos pueden tener consecuencias muy graves. Se recomienda implementar estrategias adicionales para abordar este desequilibrio, como ajustes en los umbrales de clasificaciÃ³n, tÃ©cnicas de muestreo o ponderaciÃ³n de clases."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}